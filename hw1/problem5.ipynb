{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n#Test the convolutions with 1 image, to put in the article\\n# Test\\ndf_train = pd.read_csv('train.csv')\\nimg = df_train.iloc[40,:].values[1:]\\nimg = np.reshape(img,(28,28))\\nplt.imshow(img, cmap='gray')\\nplt.show()\\nprint(img.shape)\\nplt.savefig('images/original_image.png', format='png', dpi=1200)\\n\\n# Test with a convolution of 16 filters of size 3x3\\nmy_conv = ConvolutionLayer(32,3)\\noutput = my_conv.forward_prop(img)\\n# See the dimensions of the output volume, they follow the usual formula\\nprint(output.shape)\\n\\n# Plot 16th volume after the convolution\\nplt.imshow(output[:,:,15], cmap='gray')\\nplt.show()\\nplt.savefig('images/image_convolved.png', format='png', dpi=1200)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Riccardo Andreoni\n",
    "Title: Implementation of Convolutional Neural Network from scratch.\n",
    "File: utils.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ConvolutionLayer:\n",
    "    def __init__(self, kernel_num, kernel_size):\n",
    "        \"\"\"\n",
    "        Constructor takes as input the number of kernels and their size. I assume only squared filters of size kernel_size x kernel_size\n",
    "        \"\"\"\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_size = kernel_size\n",
    "        # Generate random filters of shape (kernel_num, kernel_size, kernel_size). Divide by kernel_size^2 for weight normalization\n",
    "        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size**2)\n",
    "\n",
    "    def patches_generator(self, image):\n",
    "        \"\"\"\n",
    "        Divide the input image in patches to be used during convolution.\n",
    "        Yields the tuples containing the patches and their coordinates.\n",
    "        \"\"\"\n",
    "        # Extract image height and width\n",
    "        image_h, image_w = image.shape\n",
    "        self.image = image\n",
    "        # The number of patches, given a fxf filter is h-f+1 for height and w-f+1 for width\n",
    "        for h in range(image_h-self.kernel_size+1):\n",
    "            for w in range(image_w-self.kernel_size+1):\n",
    "                patch = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n",
    "                yield patch, h, w\n",
    "    \n",
    "    def forward_prop(self, image):\n",
    "        \"\"\"\n",
    "        Perform forward propagation for the convolutional layer.\n",
    "        \"\"\"\n",
    "        # Extract image height and width\n",
    "        image_h, image_w = image.shape\n",
    "        # Initialize the convolution output volume of the correct size\n",
    "        convolution_output = np.zeros((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_num))\n",
    "        # Unpack the generator\n",
    "        for patch, h, w in self.patches_generator(image):\n",
    "            # Perform convolution for each patch\n",
    "            convolution_output[h,w] = np.sum(patch*self.kernels, axis=(1,2))\n",
    "        return convolution_output\n",
    "    \n",
    "    def back_prop(self, dE_dY, alpha):\n",
    "        \"\"\"\n",
    "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
    "        to the kernels' weights.\n",
    "        dE_dY comes from the following layer, typically max pooling layer.\n",
    "        It updates the kernels' weights\n",
    "        \"\"\"\n",
    "        # Initialize gradient of the loss function with respect to the kernel weights\n",
    "        dE_dk = np.zeros(self.kernels.shape)\n",
    "        for patch, h, w in self.patches_generator(self.image):\n",
    "            for f in range(self.kernel_num):\n",
    "                dE_dk[f] += patch * dE_dY[h, w, f]\n",
    "        # Update the parameters\n",
    "        self.kernels -= alpha*dE_dk\n",
    "        return dE_dk\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "    def __init__(self, kernel_size):\n",
    "        \"\"\"\n",
    "        Constructor takes as input the size of the kernel\n",
    "        \"\"\"\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def patches_generator(self, image):\n",
    "        \"\"\"\n",
    "        Divide the input image in patches to be used during pooling.\n",
    "        Yields the tuples containing the patches and their coordinates.\n",
    "        \"\"\"\n",
    "        # Compute the ouput size\n",
    "        output_h = image.shape[0] // self.kernel_size\n",
    "        output_w = image.shape[1] // self.kernel_size\n",
    "        self.image = image\n",
    "\n",
    "        for h in range(output_h):\n",
    "            for w in range(output_w):\n",
    "                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
    "                yield patch, h, w\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        image_h, image_w, num_kernels = image.shape\n",
    "        max_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n",
    "        for patch, h, w in self.patches_generator(image):\n",
    "            max_pooling_output[h,w] = np.amax(patch, axis=(0,1))\n",
    "        return max_pooling_output\n",
    "\n",
    "    #Starts back prop for the convolutional layer. He does not update the weights of the filter.\n",
    "    def back_prop(self, dE_dY):\n",
    "        \"\"\"\n",
    "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
    "        to the kernels' weights.\n",
    "        dE_dY comes from the following layer, typically softmax.\n",
    "        There are no weights to update, but the output is needed to update the weights of the convolutional layer.\n",
    "        \"\"\"\n",
    "        dE_dk = np.zeros(self.image.shape)\n",
    "        for patch,h,w in self.patches_generator(self.image):\n",
    "            image_h, image_w, num_kernels = patch.shape\n",
    "            max_val = np.amax(patch, axis=(0,1))\n",
    "\n",
    "            for idx_h in range(image_h):\n",
    "                for idx_w in range(image_w):\n",
    "                    for idx_k in range(num_kernels):\n",
    "                        if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n",
    "                            dE_dk[h*self.kernel_size+idx_h, w*self.kernel_size+idx_w, idx_k] = dE_dY[h,w,idx_k]\n",
    "        return dE_dk\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    \"\"\"\n",
    "    Takes the volume coming from convolutional & pooling layers. It flattens it and it uses it in the next layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        # Initiallize weights and biases\n",
    "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
    "        self.bias = np.zeros(output_units)\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        self.original_shape = image.shape # stored for backprop\n",
    "        # Flatten the image\n",
    "        image_flattened = image.flatten()\n",
    "        self.flattened_input = image_flattened # stored for backprop\n",
    "        # Perform matrix multiplication and add bias\n",
    "        first_output = np.dot(image_flattened, self.weight) + self.bias\n",
    "        self.output = first_output\n",
    "        # Apply softmax activation\n",
    "        softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n",
    "        return softmax_output\n",
    "\n",
    "    #Back prop for the softmax layer\n",
    "    def back_prop(self, dE_dY, alpha):\n",
    "        for i, gradient in enumerate(dE_dY):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "            transformation_eq = np.exp(self.output)\n",
    "            S_total = np.sum(transformation_eq)\n",
    "\n",
    "            # Compute gradients with respect to output (Z)\n",
    "            dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
    "            dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
    "\n",
    "            # Compute gradients of output Z with respect to weight, bias, input\n",
    "            dZ_dw = self.flattened_input\n",
    "            dZ_db = 1\n",
    "            dZ_dX = self.weight\n",
    "\n",
    "            # Gradient of loss with respect ot output\n",
    "            dE_dZ = gradient * dY_dZ\n",
    "\n",
    "            # Gradient of loss with respect to weight, bias, input\n",
    "            dE_dw = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
    "            dE_db = dE_dZ * dZ_db\n",
    "            dE_dX = dZ_dX @ dE_dZ\n",
    "\n",
    "            # Update parameters\n",
    "            self.weight -= alpha*dE_dw\n",
    "            self.bias -= alpha*dE_db\n",
    "\n",
    "            return dE_dX.reshape(self.original_shape)\n",
    "\n",
    "def CNN_forward(image, label, layers):\n",
    "    output = image/255.\n",
    "    for layer in layers:\n",
    "        output = layer.forward_prop(output)\n",
    "    # Compute loss (cross-entropy) and accuracy\n",
    "    loss = -np.log(output[label])\n",
    "    accuracy = 1 if np.argmax(output) == label else 0\n",
    "    return output, loss, accuracy\n",
    "\n",
    "#Starts the back prop algorithm\n",
    "def CNN_backprop(gradient, layers, alpha=0.05):\n",
    "    grad_back = gradient\n",
    "    for layer in layers[::-1]:\n",
    "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
    "            grad_back = layer.back_prop(grad_back, alpha)\n",
    "        elif type(layer) == MaxPoolingLayer:\n",
    "            grad_back = layer.back_prop(grad_back)\n",
    "    return grad_back\n",
    "\n",
    "\n",
    "def CNN_training(image, label, layers, alpha=0.05):\n",
    "    # Forward step\n",
    "    output, loss, accuracy = CNN_forward(image, label, layers)\n",
    "\n",
    "    # Initial gradient\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1/output[label]\n",
    "\n",
    "    # Backprop step\n",
    "    gradient_back = CNN_backprop(gradient, layers, alpha)\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#Test the convolutions with 1 image, to put in the article\n",
    "# Test\n",
    "df_train = pd.read_csv('train.csv')\n",
    "img = df_train.iloc[40,:].values[1:]\n",
    "img = np.reshape(img,(28,28))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(img.shape)\n",
    "plt.savefig('images/original_image.png', format='png', dpi=1200)\n",
    "\n",
    "# Test with a convolution of 16 filters of size 3x3\n",
    "my_conv = ConvolutionLayer(32,3)\n",
    "output = my_conv.forward_prop(img)\n",
    "# See the dimensions of the output volume, they follow the usual formula\n",
    "print(output.shape)\n",
    "\n",
    "# Plot 16th volume after the convolution\n",
    "plt.imshow(output[:,:,15], cmap='gray')\n",
    "plt.show()\n",
    "plt.savefig('images/image_convolved.png', format='png', dpi=1200)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 1.9074583516227934, accuracy 41\n",
      "Step 201. For the last 100 steps: average loss 0.9999715587338477, accuracy 67\n",
      "Step 301. For the last 100 steps: average loss 0.7953451289227299, accuracy 74\n",
      "Step 401. For the last 100 steps: average loss 0.771423942617923, accuracy 75\n",
      "Step 501. For the last 100 steps: average loss 0.7922585494913302, accuracy 80\n",
      "Step 601. For the last 100 steps: average loss 0.605972297006464, accuracy 83\n",
      "Step 701. For the last 100 steps: average loss 0.7730509848107426, accuracy 77\n",
      "Step 801. For the last 100 steps: average loss 0.66395111930332, accuracy 80\n",
      "Step 901. For the last 100 steps: average loss 0.6928875838158254, accuracy 80\n",
      "Step 1001. For the last 100 steps: average loss 0.7193342512024327, accuracy 80\n",
      "Step 1101. For the last 100 steps: average loss 0.37555507906904223, accuracy 86\n",
      "Step 1201. For the last 100 steps: average loss 0.6838366042566761, accuracy 86\n",
      "Step 1301. For the last 100 steps: average loss 0.8065782297333071, accuracy 81\n",
      "Step 1401. For the last 100 steps: average loss 0.37357996576458546, accuracy 93\n",
      "Step 1501. For the last 100 steps: average loss 0.7998453978165643, accuracy 82\n",
      "Step 1601. For the last 100 steps: average loss 0.6325124258366553, accuracy 84\n",
      "Step 1701. For the last 100 steps: average loss 0.706498194818986, accuracy 84\n",
      "Step 1801. For the last 100 steps: average loss 0.391335362984185, accuracy 90\n",
      "Step 1901. For the last 100 steps: average loss 0.40283288989519717, accuracy 89\n",
      "Step 2001. For the last 100 steps: average loss 0.40601291289412217, accuracy 91\n",
      "Step 2101. For the last 100 steps: average loss 0.3742295117604173, accuracy 89\n",
      "Step 2201. For the last 100 steps: average loss 0.5022165346475113, accuracy 84\n",
      "Step 2301. For the last 100 steps: average loss 0.3131271153733828, accuracy 89\n",
      "Step 2401. For the last 100 steps: average loss 0.4930054630489572, accuracy 88\n",
      "Step 2501. For the last 100 steps: average loss 0.9233468023968233, accuracy 79\n",
      "Step 2601. For the last 100 steps: average loss 1.0594535547755703, accuracy 84\n",
      "Step 2701. For the last 100 steps: average loss 1.0167552841710288, accuracy 79\n",
      "Step 2801. For the last 100 steps: average loss 1.0761093423435948, accuracy 75\n",
      "Step 2901. For the last 100 steps: average loss 0.5311891429417092, accuracy 81\n",
      "Step 3001. For the last 100 steps: average loss 1.4115846762679487, accuracy 73\n",
      "Step 3101. For the last 100 steps: average loss 1.1814810457734426, accuracy 79\n",
      "Step 3201. For the last 100 steps: average loss 1.2947103863824048, accuracy 82\n",
      "Step 3301. For the last 100 steps: average loss 1.183973097800977, accuracy 79\n",
      "Step 3401. For the last 100 steps: average loss 1.1533474394076486, accuracy 79\n",
      "Step 3501. For the last 100 steps: average loss 0.8278988249769184, accuracy 84\n",
      "Step 3601. For the last 100 steps: average loss 0.9738611634129143, accuracy 81\n",
      "Step 3701. For the last 100 steps: average loss 1.70669694510076, accuracy 76\n",
      "Step 3801. For the last 100 steps: average loss 1.705817034897915, accuracy 73\n",
      "Step 3901. For the last 100 steps: average loss 1.2164474535672194, accuracy 78\n",
      "Step 4001. For the last 100 steps: average loss 1.6692231946140792, accuracy 78\n",
      "Step 4101. For the last 100 steps: average loss 0.8866788911165354, accuracy 86\n",
      "Step 4201. For the last 100 steps: average loss 1.4498003981257401, accuracy 81\n",
      "Step 4301. For the last 100 steps: average loss 1.2939453825182028, accuracy 82\n",
      "Step 4401. For the last 100 steps: average loss 1.8577440563650387, accuracy 84\n",
      "Step 4501. For the last 100 steps: average loss 1.1583940002358029, accuracy 80\n",
      "Step 4601. For the last 100 steps: average loss 1.7243235286797371, accuracy 84\n",
      "Step 4701. For the last 100 steps: average loss 0.6114732183605001, accuracy 88\n",
      "Step 4801. For the last 100 steps: average loss 2.3257919774780595, accuracy 80\n",
      "Step 4901. For the last 100 steps: average loss 3.3705099234633438, accuracy 77\n",
      "Epoch 2 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 1.4328143624998066, accuracy 81\n",
      "Step 201. For the last 100 steps: average loss 0.6693111332544981, accuracy 89\n",
      "Step 301. For the last 100 steps: average loss 2.1882852861631723, accuracy 83\n",
      "Step 401. For the last 100 steps: average loss 1.7664201254124179, accuracy 87\n",
      "Step 501. For the last 100 steps: average loss 2.760915373026518, accuracy 84\n",
      "Step 601. For the last 100 steps: average loss 2.3678590025384048, accuracy 83\n",
      "Step 701. For the last 100 steps: average loss 4.525606026469108, accuracy 82\n",
      "Step 801. For the last 100 steps: average loss 2.7390943898767417, accuracy 82\n",
      "Step 901. For the last 100 steps: average loss 2.4627861336143226, accuracy 82\n",
      "Step 1001. For the last 100 steps: average loss 6.003397534310957, accuracy 78\n",
      "Step 1101. For the last 100 steps: average loss 5.574724201036826, accuracy 80\n",
      "Step 1201. For the last 100 steps: average loss 5.7220928368817745, accuracy 79\n",
      "Step 1301. For the last 100 steps: average loss 4.582385292783704, accuracy 81\n",
      "Step 1401. For the last 100 steps: average loss 5.191564718217623, accuracy 82\n",
      "Step 1501. For the last 100 steps: average loss 7.214615449931593, accuracy 75\n",
      "Step 1601. For the last 100 steps: average loss 3.744051545058461, accuracy 83\n",
      "Step 1701. For the last 100 steps: average loss 11.476969449513767, accuracy 70\n",
      "Step 1801. For the last 100 steps: average loss 12.179805450918117, accuracy 73\n",
      "Step 1901. For the last 100 steps: average loss 6.6980783164384015, accuracy 79\n",
      "Step 2001. For the last 100 steps: average loss 9.128009244006062, accuracy 80\n",
      "Step 2101. For the last 100 steps: average loss 10.60575690823262, accuracy 75\n",
      "Step 2201. For the last 100 steps: average loss 4.894606898125959, accuracy 86\n",
      "Step 2301. For the last 100 steps: average loss 11.195423699520653, accuracy 78\n",
      "Step 2401. For the last 100 steps: average loss 7.223108345592222, accuracy 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/65hzhjw544v8bhbvyq78jf9c0000gn/T/ipykernel_39092/534009194.py:142: RuntimeWarning: overflow encountered in multiply\n",
      "  dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
      "/var/folders/n8/65hzhjw544v8bhbvyq78jf9c0000gn/T/ipykernel_39092/534009194.py:142: RuntimeWarning: overflow encountered in scalar power\n",
      "  dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
      "/var/folders/n8/65hzhjw544v8bhbvyq78jf9c0000gn/T/ipykernel_39092/534009194.py:142: RuntimeWarning: invalid value encountered in divide\n",
      "  dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
      "/var/folders/n8/65hzhjw544v8bhbvyq78jf9c0000gn/T/ipykernel_39092/534009194.py:143: RuntimeWarning: overflow encountered in scalar power\n",
      "  dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2501. For the last 100 steps: average loss 12.932889032910715, accuracy 76\n",
      "Step 2601. For the last 100 steps: average loss 12.696044804465707, accuracy 81\n",
      "Step 2701. For the last 100 steps: average loss 11.743564369459744, accuracy 74\n",
      "Step 2801. For the last 100 steps: average loss 10.726836775746895, accuracy 78\n",
      "Step 2901. For the last 100 steps: average loss 12.047987585006457, accuracy 78\n",
      "Step 3001. For the last 100 steps: average loss 8.452689688725464, accuracy 90\n",
      "Step 3101. For the last 100 steps: average loss 13.222381936286725, accuracy 79\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 69\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 3\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Epoch 3 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 18\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 8\n",
      "Epoch 4 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 301. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 401. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 601. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 701. For the last 100 steps: average loss nan, accuracy 16\n",
      "Step 801. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 901. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1001. For the last 100 steps: average loss nan, accuracy 14\n",
      "Step 1101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1201. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1401. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 1501. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 1601. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 1801. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 1901. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2001. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2201. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 2301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2501. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 2601. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 2701. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 2801. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 2901. For the last 100 steps: average loss nan, accuracy 12\n",
      "Step 3001. For the last 100 steps: average loss nan, accuracy 15\n",
      "Step 3101. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 3201. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3301. For the last 100 steps: average loss nan, accuracy 10\n",
      "Step 3401. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 3501. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 3601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 3701. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3801. For the last 100 steps: average loss nan, accuracy 13\n",
      "Step 3901. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4001. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4101. For the last 100 steps: average loss nan, accuracy 7\n",
      "Step 4201. For the last 100 steps: average loss nan, accuracy 6\n",
      "Step 4301. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4401. For the last 100 steps: average loss nan, accuracy 8\n",
      "Step 4501. For the last 100 steps: average loss nan, accuracy 5\n",
      "Step 4601. For the last 100 steps: average loss nan, accuracy 11\n",
      "Step 4701. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4801. For the last 100 steps: average loss nan, accuracy 9\n",
      "Step 4901. For the last 100 steps: average loss nan, accuracy 8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Riccardo Andreoni\n",
    "Title: Implementation of Convolutional Neural Network from scratch.\n",
    "File: main.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "  # Load training data\n",
    "  (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "  X_train = X_train[:5000]\n",
    "  y_train = y_train[:5000]\n",
    "\n",
    "  # Define the network\n",
    "  layers = [\n",
    "    #He does not use a non linear activation function after the convolutional layer\n",
    "    ConvolutionLayer(16,3), # layer with 8 3x3 filters, output (26,26,16)\n",
    "    MaxPoolingLayer(2), # pooling layer 2x2, output (13,13,16)\n",
    "    SoftmaxLayer(13*13*16, 10) # softmax layer with 13*13*16 input and 10 output\n",
    "    ] \n",
    "\n",
    "  #Loop through the data 4 times\n",
    "  for epoch in range(4):\n",
    "    print('Epoch {} ->'.format(epoch+1))\n",
    "    # Shuffle training data\n",
    "    permutation = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "    # Training the CNN\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    #Loop through each image and train for each one\n",
    "    for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
    "      if i % 100 == 0: # Every 100 examples\n",
    "        print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "      loss_1, accuracy_1 = CNN_training(image, label, layers)\n",
    "      loss += loss_1\n",
    "      accuracy += accuracy_1\n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  main()\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
